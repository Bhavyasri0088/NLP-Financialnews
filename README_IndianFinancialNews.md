# Indian Financial News — README (auto-generated from notebook analysis)

**NOTE:** This README was generated by scanning the notebook `IndianFinancialNews_merged_final.ipynb` for code patterns, imports, and comments. Statements below are *inferred* from the notebook contents and marked with a confidence level (High / Medium / Low).

## First notebook markdown (summary)

# Indian Financial News — **Merged & Upgraded Notebook**

This is a **single, evaluator-ready notebook** that merges your original polished work with the requested enhancements, step by step:

**What’s included**
- Clear **project overview** and **data dictionary**

## Files read / written (detected)

- No explicit filenames detected via `pd.read_csv()` or `open()`.

- No explicit output filenames detected via `to_csv`, `joblib.dump`, or `pickle.dump`.


## Key imports detected (partial)

joblib, matplotlib.pyplot, nltk, nltk.corpus, nltk.stem, numpy, os, pandas, seaborn, shap, sklearn.ensemble, sklearn.feature_extraction.text, sklearn.linear_model, sklearn.metrics, sklearn.model_selection, sklearn.naive_bayes, sklearn.pipeline, tqdm, wordcloud, xgboost


## Step-by-step updates you (likely) made — inferred with confidence

1. Loaded and merged one or more CSV datasets using `pandas.read_csv()` (1 calls detected).  
   - Confidence: **Medium**

2. Handled missing values using `dropna()` / `fillna()` (dropna: 1, fillna: 0).  
   - Confidence: **Medium**

3. Transformed text into numeric features using TF-IDF / CountVectorizer (tfidf count: 14, countvec: 0).  
   - Confidence: **High**

4. Applied NLP preprocessing: NLTK + lemmatization.  
   - Confidence: **High**

5. Trained one or more classification models: LogisticRegression, RandomForestClassifier, XGBoost (XGBClassifier), Naive Bayes (MultinomialNB).  
   - Confidence: **High**

6. Evaluated models with `classification_report`, `confusion_matrix`, and/or ROC-AUC metrics.  
   - Confidence: **High**

7. Persisted model(s) / artifacts using `pickle` / `joblib` (1 matches).  
   - Confidence: **Medium**

8. Performed EDA and visualizations using matplotlib/seaborn (23 matplotlib matches, 3 seaborn matches).  
   - Confidence: **High**


## Skills & technologies used (inferred)

- Pandas — data loading, cleaning, merging, aggregation
- NumPy — numerical operations and arrays
- Scikit-learn — feature extraction, model training, pipelines, evaluation
- NLTK — tokenization, stopwords, stemming/lemmatization
- XGBoost — gradient boosted trees
- Matplotlib / Seaborn — EDA visualizations
- Model persistence — pickle / joblib
- Exploratory Data Analysis (EDA)
- Text preprocessing and feature engineering (TF-IDF / CountVectorizer)
- Classification modeling and evaluation
- Hyperparameter tuning and cross-validation
- Reproducibility practices (saving artifacts, setting random seeds)

## How to reproduce / run

1. Create a virtual environment and install packages (example):
   ```bash
   python -m venv venv
   source venv/bin/activate
   pip install pandas numpy scikit-learn matplotlib seaborn nltk spacy xgboost joblib
   ```
2. Ensure any CSV/data files referenced in the notebook are in the paths expected by `pd.read_csv()` or change the paths accordingly.
3. Run the notebook from top to bottom. Check for cells that download models or set `DATA_PATHS`/`CANDIDATE_PATHS` variables.
4. If models/vectorizers are saved to disk, you can load them with `joblib.load()` or `pickle.load()` for inference.
