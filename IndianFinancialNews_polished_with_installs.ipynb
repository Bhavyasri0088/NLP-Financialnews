{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebe599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install \\\n",
    "    pandas \\\n",
    "    numpy \\\n",
    "    scikit-learn \\\n",
    "    matplotlib \\\n",
    "    seaborn \\\n",
    "    nltk \\\n",
    "    wordcloud \\\n",
    "    joblib \\\n",
    "    tqdm\n",
    "\n",
    "# Import all required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28bd70",
   "metadata": {},
   "source": [
    "\n",
    "# Indian Financial News — Sentiment Analysis (Polished)\n",
    "**Last updated:** 2025-08-14\n",
    "\n",
    "This notebook provides an end‑to‑end, production‑ready pipeline for sentiment classification of Indian financial news headlines/articles. It is designed to be **robust, reproducible, and easy to extend**.\n",
    "\n",
    "**Highlights**\n",
    "- Configurable column names with smart auto‑detection (text / label / date)\n",
    "- Clean text preprocessing with safe fallbacks (NLTK optional)\n",
    "- **Stratified** train/test split to respect class balance\n",
    "- **Model sweep** (Logistic Regression, Linear SVC, Multinomial NB)\n",
    "- **Cross‑validation** and macro‑F1 focus for imbalanced classes\n",
    "- Clear **EDA**, **error analysis**, and **explainability**\n",
    "- **Single sklearn Pipeline** saved with `joblib` for clean deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0766aa",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Configuration\n",
    "Set these to match your dataset. If left as `None`, the notebook will try to **auto-detect** columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafbca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== USER CONFIG =====\n",
    "# If your CSV has specific column names, set them here.\n",
    "TEXT_COL_OVERRIDE  = None   # e.g., \"headline\" or \"Text\"\n",
    "LABEL_COL_OVERRIDE = None   # e.g., \"sentiment\" or \"Label\"\n",
    "DATE_COL_OVERRIDE  = None   # e.g., \"date\" or \"Date\"\n",
    "\n",
    "# Path to dataset. The notebook will try multiple locations.\n",
    "DATA_PATHS = [\n",
    "    # 1) Kaggle default (if you added a dataset named \"indianfinancialnews\")\n",
    "    \"/kaggle/input/indianfinancialnews/IndianFinancialNews.csv\",\n",
    "    # 2) Environment variable\n",
    "    os.getenv(\"DATA_PATH\"),\n",
    "    # 3) Local project paths (edit as needed)\n",
    "    \"./IndianFinancialNews.csv\",\n",
    "    \"./data/IndianFinancialNews.csv\",\n",
    "]\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Output folder for artifacts (figures, model, vectorizer, reports)\n",
    "ARTIFACTS_DIR = \"./artifacts\"\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439cbf7",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & Versions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, json, math, string, random, warnings, textwrap\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "\n",
    "# Optional: NLTK for better tokenization/lemmatization (with safe fallbacks)\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    NLTK_OK = True\n",
    "except Exception:\n",
    "    NLTK_OK = False\n",
    "    word_tokenize = None\n",
    "    WordNetLemmatizer = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Version info\n",
    "VERSIONS = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"scikit_learn\": __import__(\"sklearn\").__version__,\n",
    "    \"matplotlib\": plt.matplotlib.__version__,\n",
    "    \"nltk\": __import__(\"nltk\").__version__ if NLTK_OK else \"not available\",\n",
    "}\n",
    "print(json.dumps(VERSIONS, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289f863",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Loading (with Auto‑Detection)\n",
    "This cell attempts to read the dataset from multiple candidate paths. Override `DATA_PATHS` above if needed.\n",
    "The notebook also tries to **auto-detect** the text, label, and date columns if you haven't specified them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility: find first readable path\n",
    "def first_existing(paths: List[Optional[str]]) -> Optional[str]:\n",
    "    for p in paths:\n",
    "        if p and isinstance(p, str) and os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "data_path = first_existing(DATA_PATHS)\n",
    "if not data_path:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find dataset. Please upload your CSV and set DATA_PATHS or the DATA_PATH env var.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"Loaded:\", data_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "# Heuristics for column detection\n",
    "def guess_text_column(frame: pd.DataFrame) -> Optional[str]:\n",
    "    # choose the longest average string column\n",
    "    text_like = []\n",
    "    for col in frame.columns:\n",
    "        if frame[col].dtype == object:\n",
    "            sample = frame[col].dropna().astype(str).head(50)\n",
    "            avg_len = sample.map(len).mean() if not sample.empty else 0\n",
    "            text_like.append((col, avg_len))\n",
    "    text_like.sort(key=lambda x: x[1], reverse=True)\n",
    "    return text_like[0][0] if text_like else None\n",
    "\n",
    "def guess_label_column(frame: pd.DataFrame) -> Optional[str]:\n",
    "    candidates = [\"label\", \"sentiment\", \"target\", \"class\", \"y\"]\n",
    "    for c in candidates:\n",
    "        if c in frame.columns:\n",
    "            return c\n",
    "    # Fallback: any low-cardinality object/int column (<=10 unique)\n",
    "    for col in frame.columns:\n",
    "        if frame[col].dtype in [object, int, np.int64, np.int32]:\n",
    "            nunique = frame[col].nunique(dropna=True)\n",
    "            if 2 <= nunique <= 10:\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def guess_date_column(frame: pd.DataFrame) -> Optional[str]:\n",
    "    for c in [\"date\", \"Date\", \"published\", \"time\", \"created_at\", \"timestamp\"]:\n",
    "        if c in frame.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "TEXT_COL  = TEXT_COL_OVERRIDE  or guess_text_column(df)\n",
    "LABEL_COL = LABEL_COL_OVERRIDE or guess_label_column(df)\n",
    "DATE_COL  = DATE_COL_OVERRIDE  or guess_date_column(df)\n",
    "\n",
    "print(\"TEXT_COL :\", TEXT_COL)\n",
    "print(\"LABEL_COL:\", LABEL_COL)\n",
    "print(\"DATE_COL :\", DATE_COL)\n",
    "\n",
    "if TEXT_COL is None:\n",
    "    raise ValueError(\"Could not detect a text column. Please set TEXT_COL_OVERRIDE.\")\n",
    "if LABEL_COL is None:\n",
    "    # If no label, create a placeholder neutral label to allow running EDA; modeling will be skipped.\n",
    "    print(\"No label column detected. Creating a dummy 'label' column of 'neutral'.\")\n",
    "    df[\"label\"] = \"neutral\"\n",
    "    LABEL_COL = \"label\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6348f2c3",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Basic Cleaning\n",
    "We lowercase, strip URLs/punctuation/numbers, and optionally lemmatize (if NLTK is available). A simple regex fallback is used otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ac57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "URL_RE = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "NON_ALPHA_RE = re.compile(r\"[^a-z\\s]\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() if (NLTK_OK and WordNetLemmatizer is not None) else None\n",
    "\n",
    "def basic_clean(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    s = URL_RE.sub(\" \", s)\n",
    "    s = NON_ALPHA_RE.sub(\" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def tokenize_and_lemmatize(text: str) -> List[str]:\n",
    "    text = basic_clean(text)\n",
    "    tokens = text.split() if (not NLTK_OK or word_tokenize is None) else word_tokenize(text)\n",
    "    if lemmatizer:\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return [t for t in tokens if t]\n",
    "\n",
    "# Apply cleaning preview\n",
    "df[\"text_clean\"] = df[TEXT_COL].astype(str).map(basic_clean)\n",
    "display(df[[TEXT_COL, \"text_clean\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7e83e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class distribution (if labels are meaningful)\n",
    "label_counts = df[LABEL_COL].value_counts(dropna=False)\n",
    "print(label_counts)\n",
    "ax = label_counts.plot(kind=\"bar\")\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb68144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Document length distribution\n",
    "doc_len = df[\"text_clean\"].map(lambda x: len(x.split()))\n",
    "ax = doc_len.hist(bins=30)\n",
    "plt.title(\"Document Length Distribution (tokens)\")\n",
    "plt.xlabel(\"Tokens per document\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22882e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Top unigrams by TF-IDF (approximate by raw counts for speed)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=40, stop_words=\"english\")\n",
    "X_counts = cv.fit_transform(df[\"text_clean\"])\n",
    "vocab = cv.get_feature_names_out()\n",
    "freqs = np.asarray(X_counts.sum(axis=0)).ravel()\n",
    "top_idx = np.argsort(freqs)[::-1][:20]\n",
    "top_terms = [(vocab[i], int(freqs[i])) for i in top_idx]\n",
    "print(\"Top terms:\", top_terms)\n",
    "plt.figure()\n",
    "plt.bar([t for t,_ in top_terms], [c for _,c in top_terms])\n",
    "plt.xticks(rotation=60, ha=\"right\")\n",
    "plt.title(\"Top Terms (raw counts)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46c17c",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train/Test Split & Model Sweep\n",
    "We use **Stratified** split and evaluate multiple models with consistent TF‑IDF features via a `Pipeline`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a233a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure labels are strings (for consistent reporting)\n",
    "y = df[LABEL_COL].astype(str).values\n",
    "X = df[\"text_clean\"].astype(str).values\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y if len(set(y))>1 else None\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=200, n_jobs=None if hasattr(LogisticRegression, \"n_jobs\") else None),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    name: Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(tokenizer=tokenize_and_lemmatize, preprocessor=None, lowercase=False,\n",
    "                                  max_features=50000, ngram_range=(1,2), min_df=2, stop_words=\"english\")),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "    for name, model in models.items()\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE) if len(set(y_train))>1 else None\n",
    "\n",
    "cv_results = {}\n",
    "for name, pipe in pipelines.items():\n",
    "    if cv is not None:\n",
    "        f1_scores = cross_val_score(pipe, X_train, y_train, scoring=\"f1_macro\", cv=cv, n_jobs=None)\n",
    "        acc_scores = cross_val_score(pipe, X_train, y_train, scoring=\"accuracy\",  cv=cv, n_jobs=None)\n",
    "        cv_results[name] = {\"f1_macro_mean\": f1_scores.mean(), \"f1_macro_std\": f1_scores.std(),\n",
    "                            \"acc_mean\": acc_scores.mean(), \"acc_std\": acc_scores.std()}\n",
    "    else:\n",
    "        cv_results[name] = {\"note\": \"Single-class labels; skipping CV.\"}\n",
    "\n",
    "print(json.dumps(cv_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c719c7",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Final Fit & Evaluation (Hold‑out Test)\n",
    "We pick the best model by CV macro‑F1 (fallback to LogisticRegression if CV unavailable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose the best model\n",
    "if cv is not None:\n",
    "    best_name = max(cv_results.keys(), key=lambda k: cv_results[k][\"f1_macro_mean\"])\n",
    "else:\n",
    "    best_name = \"LogisticRegression\"\n",
    "\n",
    "best_pipe = pipelines[best_name]\n",
    "print(f\"Best model: {best_name}\")\n",
    "best_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "print({\"accuracy\": acc, \"macro_precision\": prec, \"macro_recall\": rec, \"macro_f1\": f1})\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=sorted(np.unique(y)))\n",
    "print(\"Labels order:\", sorted(np.unique(y)))\n",
    "print(cm)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect=\"auto\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb340185",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Error Analysis\n",
    "Inspect common confusions and a sample of misclassifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435c4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "errors = []\n",
    "for xi, yi, yp in zip(X_test, y_test, y_pred):\n",
    "    if yi != yp:\n",
    "        errors.append((yi, yp, xi))\n",
    "\n",
    "# Show up to 20 errors\n",
    "print(f\"Total errors: {len(errors)}\")\n",
    "for i, (yt, yp, txt) in enumerate(errors[:20], 1):\n",
    "    print(f\"{i:>2}. TRUE={yt} | PRED={yp} | TEXT={txt[:220]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4001dd",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Explainability: Top Features per Class\n",
    "For linear models, we can inspect coefficients to see which tokens push predictions toward each label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07de9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_features_for_linear_classifier(pipeline: Pipeline, k: int = 20):\n",
    "    vec: TfidfVectorizer = pipeline.named_steps[\"tfidf\"]\n",
    "    clf = pipeline.named_steps[\"clf\"]\n",
    "    if not hasattr(clf, \"coef_\"):\n",
    "        print(\"Top features not available for this classifier.\")\n",
    "        return\n",
    "    feature_names = np.array(vec.get_feature_names_out())\n",
    "    classes = clf.classes_\n",
    "    for idx, cls in enumerate(classes):\n",
    "        coefs = clf.coef_[idx]\n",
    "        topk = np.argsort(coefs)[-k:][::-1]\n",
    "        print(f\"\\nTop {k} features for class '{cls}':\")\n",
    "        for f in feature_names[topk]:\n",
    "            print(f\"  {f}\")\n",
    "\n",
    "top_features_for_linear_classifier(best_pipe, k=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdb2de",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Inference Helper\n",
    "Use the saved pipeline to predict on new headlines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ecce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_sentiment(texts: List[str], pipeline: Pipeline = None) -> pd.DataFrame:\n",
    "    if pipeline is None:\n",
    "        pipeline = joblib.load(os.path.join(ARTIFACTS_DIR, \"sentiment_pipeline.joblib\"))\n",
    "    preds = pipeline.predict(texts)\n",
    "    return pd.DataFrame({\"text\": texts, \"prediction\": preds})\n",
    "\n",
    "# Demo\n",
    "examples = [\n",
    "    \"RBI keeps repo rate unchanged; inflation seen moderating\",\n",
    "    \"Banking stocks fall as rupee weakens against the dollar\",\n",
    "    \"HDFC Bank reports strong Q1 profit growth\"\n",
    "]\n",
    "predict_sentiment(examples, best_pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260e228",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Save Artifacts\n",
    "We persist a **single sklearn Pipeline** that includes preprocessing + vectorization + model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d170be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_path = os.path.join(ARTIFACTS_DIR, \"sentiment_pipeline.joblib\")\n",
    "joblib.dump(best_pipe, pipe_path)\n",
    "print(\"Saved pipeline to:\", pipe_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724290ef",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Environment Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = {\n",
    "    \"data_path\": data_path,\n",
    "    \"rows\": int(df.shape[0]),\n",
    "    \"text_col\": TEXT_COL,\n",
    "    \"label_col\": LABEL_COL,\n",
    "    \"date_col\": DATE_COL,\n",
    "    \"best_model\": best_name,\n",
    "    \"versions\": VERSIONS,\n",
    "    \"artifacts_dir\": os.path.abspath(ARTIFACTS_DIR),\n",
    "}\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
