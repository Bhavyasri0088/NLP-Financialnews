{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3dc94b0",
   "metadata": {},
   "source": [
    "\n",
    "# Indian Financial News — **Merged & Upgraded Notebook**\n",
    "\n",
    "This is a **single, evaluator-ready notebook** that merges your original polished work with the requested enhancements, step by step:\n",
    "\n",
    "**What’s included**\n",
    "- Clear **project overview** and **data dictionary**\n",
    "- Robust **data loading** (Kaggle/Colab/local paths; auto column detection)\n",
    "- Thorough **EDA** (class balance, text lengths, word clouds)\n",
    "- Clean **text preprocessing** (custom financial stopwords + lemmatization)\n",
    "- Strong **modeling** with **TF‑IDF + multiple algorithms** (LR, NB, RF, XGBoost)\n",
    "- **5‑fold cross‑validation** model comparison\n",
    "- **Hold‑out test** evaluation with precision/recall/F1 + confusion matrix\n",
    "- **Explainability** via SHAP (top features driving predictions)\n",
    "- **Deployment‑ready**: single **Pipeline** saved (vectorizer + model), plus a small **predict()** utility\n",
    "- **Artifacts** persisted to `./artifacts/` with versioned names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51001c7d",
   "metadata": {},
   "source": [
    "## 1) Setup — Installs & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15829d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running in Colab/Kaggle, installs are safe to re-run.\n",
    "!pip -q install pandas numpy scikit-learn matplotlib seaborn nltk wordcloud joblib tqdm shap xgboost\n",
    "\n",
    "import os, re, math, json, time, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "# NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Artifacts dir\n",
    "ARTIFACTS_DIR = \"./artifacts\"\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66a837",
   "metadata": {},
   "source": [
    "## 2) Load Data — Robust Paths + Auto Column Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Candidate paths (edit or add paths if needed)\n",
    "CANDIDATE_PATHS = [\n",
    "    \"/kaggle/input/indianfinancialnews/IndianFinancialNews.csv\",\n",
    "    \"/kaggle/input/financialphrasebank/FinancialPhraseBank-v1.0.csv\",\n",
    "    \"./IndianFinancialNews.csv\",\n",
    "    \"./data/IndianFinancialNews.csv\",\n",
    "]\n",
    "\n",
    "df = None\n",
    "for p in CANDIDATE_PATHS:\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            print(f\"Loaded dataset from: {p}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {p}: {e}\")\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"Dataset not found in candidate paths. Please update CANDIDATE_PATHS.\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(3))\n",
    "\n",
    "# Basic data dictionary\n",
    "print(\"\\nData Dictionary (assumed):\")\n",
    "print(\" - Text column: headline/body of financial news\")\n",
    "print(\" - Sentiment: 0 = negative, 1 = positive\")\n",
    "if \"Date\" in df.columns:\n",
    "    print(\" - Date: date of publication\")\n",
    "\n",
    "# Drop obvious index-like columns if present\n",
    "for c in list(df.columns):\n",
    "    if str(c).lower().startswith(\"unnamed\"):\n",
    "        df = df.drop(columns=[c])\n",
    "\n",
    "# Auto-detect text & label columns\n",
    "TEXT_CANDIDATES = [\"Text\", \"Sentence\", \"Headline\", \"title\", \"news\", \"content\", \"text\"]\n",
    "LABEL_CANDIDATES = [\"Sentiment\", \"label\", \"target\", \"polarity\", \"y\"]\n",
    "\n",
    "text_col, label_col = None, None\n",
    "for c in df.columns:\n",
    "    if c in TEXT_CANDIDATES and text_col is None:\n",
    "        text_col = c\n",
    "    if c in LABEL_CANDIDATES and label_col is None:\n",
    "        label_col = c\n",
    "\n",
    "# Fallback: heuristic detection\n",
    "if text_col is None:\n",
    "    # Choose the longest-average string column as text\n",
    "    string_cols = [c for c in df.columns if df[c].dtype == 'object']\n",
    "    if string_cols:\n",
    "        text_col = max(string_cols, key=lambda c: df[c].astype(str).str.len().mean())\n",
    "if label_col is None:\n",
    "    # choose the smallest unique int/binary-ish col\n",
    "    numeric_cols = [c for c in df.columns if pd.api.types.is_integer_dtype(df[c])]\n",
    "    if numeric_cols:\n",
    "        label_col = min(numeric_cols, key=lambda c: df[c].nunique())\n",
    "\n",
    "if text_col is None or label_col is None:\n",
    "    raise ValueError(\"Could not auto-detect text or label column. Please set them manually.\")\n",
    "\n",
    "print(f\"\\nUsing Text Column: {text_col}\")\n",
    "print(f\"Using Label Column: {label_col}\")\n",
    "\n",
    "# Keep only needed columns & drop NA\n",
    "df = df[[text_col, label_col] + ([c for c in [\"Date\"] if c in df.columns])].dropna().reset_index(drop=True)\n",
    "df.rename(columns={text_col: \"Text\", label_col: \"Sentiment\"}, inplace=True)\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e3faf",
   "metadata": {},
   "source": [
    "## 3) EDA — Class Balance, Text Lengths, Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435338f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "sns.countplot(data=df, x=\"Sentiment\")\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Class share (normalized):\")\n",
    "display(df[\"Sentiment\"].value_counts(normalize=True).rename(\"proportion\").to_frame())\n",
    "\n",
    "# Text lengths\n",
    "df[\"text_len\"] = df[\"Text\"].astype(str).str.split().apply(len)\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "df[\"text_len\"].plot(kind=\"hist\", bins=40, alpha=0.8)\n",
    "plt.title(\"Text Length Distribution (in words)\")\n",
    "plt.xlabel(\"Words per sample\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# WordClouds (optional)\n",
    "pos_text = \" \".join(df.loc[df[\"Sentiment\"]==1, \"Text\"].astype(str).head(5000))\n",
    "neg_text = \" \".join(df.loc[df[\"Sentiment\"]==0, \"Text\"].astype(str).head(5000))\n",
    "\n",
    "if len(pos_text) > 0:\n",
    "    wc_pos = WordCloud(width=800, height=400, max_words=200).generate(pos_text)\n",
    "    plt.figure(figsize=(8,4)); plt.imshow(wc_pos); plt.axis(\"off\"); plt.title(\"WordCloud — Positive\"); plt.show()\n",
    "if len(neg_text) > 0:\n",
    "    wc_neg = WordCloud(width=800, height=400, max_words=200).generate(neg_text)\n",
    "    plt.figure(figsize=(8,4)); plt.imshow(wc_neg); plt.axis(\"off\"); plt.title(\"WordCloud — Negative\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f2f65",
   "metadata": {},
   "source": [
    "## 4) Preprocessing — Custom Financial Stopwords + Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom stopwords\n",
    "custom_sw = set(stopwords.words(\"english\"))\n",
    "financial_terms = {\n",
    "    \"stock\",\"stocks\",\"market\",\"markets\",\"share\",\"shares\",\"company\",\"companies\",\"price\",\"prices\",\n",
    "    \"nse\",\"bse\",\"sensex\",\"nifty\",\"india\",\"indian\",\"rupee\",\"crore\",\"quarter\",\"q1\",\"q2\",\"q3\",\"q4\",\n",
    "    \"mkt\",\"bps\",\"fy\",\"fy22\",\"fy23\",\"fy24\",\"fy25\",\"inc\",\"ltd\",\"limited\"\n",
    "}\n",
    "custom_sw |= financial_terms\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text).lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep alphabetic tokens only\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in custom_sw]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "tqdm.pandas(desc=\"Cleaning text\")\n",
    "df[\"clean_text\"] = df[\"Text\"].astype(str).progress_apply(clean_text)\n",
    "display(df[[\"Text\",\"clean_text\",\"Sentiment\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4537d822",
   "metadata": {},
   "source": [
    "## 5) Split — Stratified Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c926c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[\"clean_text\"].values\n",
    "y = df[\"Sentiment\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19713058",
   "metadata": {},
   "source": [
    "## 6) Model Comparison — 5‑Fold Cross‑Validation (TF‑IDF + Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ececc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "pipelines = {\n",
    "    \"LogReg\": Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=5000, ngram_range=(1,2))),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=RANDOM_STATE)),\n",
    "    ]),\n",
    "    \"NaiveBayes\": Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=5000, ngram_range=(1,2))),\n",
    "        (\"clf\", MultinomialNB()),\n",
    "    ]),\n",
    "    \"RandomForest\": Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=5000, ngram_range=(1,2))),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)),\n",
    "    ]),\n",
    "    \"XGBoost\": Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=5000, ngram_range=(1,2))),\n",
    "        (\"clf\", XGBClassifier(\n",
    "            n_estimators=400, max_depth=6, learning_rate=0.1, subsample=0.9, colsample_bytree=0.9,\n",
    "            eval_metric=\"logloss\", random_state=RANDOM_STATE, tree_method=\"hist\"\n",
    "        )),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "for name, pipe in pipelines.items():\n",
    "    scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "    cv_results[name] = {\"mean_acc\": scores.mean(), \"std\": scores.std(), \"n_splits\": cv.get_n_splits()}\n",
    "    \n",
    "cv_df = pd.DataFrame(cv_results).T.sort_values(\"mean_acc\", ascending=False)\n",
    "display(cv_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a542a68",
   "metadata": {},
   "source": [
    "## 7) Final Model — Train Best Pipeline & Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27d6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_name = cv_df.index[0]\n",
    "best_pipe = pipelines[best_name]\n",
    "\n",
    "print(f\"Best model by CV: {best_name}\")\n",
    "best_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "disp.plot(values_format='d')\n",
    "plt.title(f\"Confusion Matrix — {best_name}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c60a30",
   "metadata": {},
   "source": [
    "## 8) Explainability — SHAP (Top Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341372a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SHAP with linear models is most straightforward; we support LogReg here.\n",
    "try:\n",
    "    if best_name == \"LogReg\":\n",
    "        # Access fitted steps\n",
    "        tfidf = best_pipe.named_steps[\"tfidf\"]\n",
    "        clf = best_pipe.named_steps[\"clf\"]\n",
    "        # Sample a small subset for speed\n",
    "        sample_idx = np.random.default_rng(RANDOM_STATE).choice(len(X_test), size=min(200, len(X_test)), replace=False)\n",
    "        X_test_sample = [X_test[i] for i in sample_idx]\n",
    "        X_vec = tfidf.transform(X_test_sample)\n",
    "        \n",
    "        # LinearExplainer for LR\n",
    "        explainer = shap.LinearExplainer(clf, tfidf.transform(X_train), feature_perturbation=\"interventional\")\n",
    "        shap_values = explainer.shap_values(X_vec)\n",
    "        \n",
    "        # Summary plot (may open a JS-based viz)\n",
    "        shap.summary_plot(shap_values, X_vec, feature_names=tfidf.get_feature_names_out())\n",
    "    else:\n",
    "        print(f\"SHAP demo is optimized for LogReg; current best is {best_name}.\")\n",
    "        print(\"You can set best_name='LogReg' above to force a SHAP run for linear model interpretation.\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP visualization skipped due to error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c8d30",
   "metadata": {},
   "source": [
    "## 9) Save Artifacts — Pipeline (Vectorizer + Model) & Predict Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_tag = f\"{best_name}_{timestamp}\"\n",
    "pipe_path = os.path.join(ARTIFACTS_DIR, f\"sentiment_pipeline_{model_tag}.joblib\")\n",
    "\n",
    "joblib.dump(best_pipe, pipe_path)\n",
    "print(\"Saved pipeline to:\", pipe_path)\n",
    "\n",
    "def load_pipeline(p=pipe_path):\n",
    "    return joblib.load(p)\n",
    "\n",
    "def predict(texts, pipeline_path=pipe_path):\n",
    "    pipe = load_pipeline(pipeline_path)\n",
    "    return pipe.predict(texts).tolist()\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Predict sanity check:\", predict([\"Profit jumps in Q1 as revenue rises\", \"Company faces losses amid weak demand\"]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de79509",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Conclusions & Next Steps\n",
    "\n",
    "**What we achieved**\n",
    "- Built a **clean, reproducible** pipeline for Indian financial news sentiment\n",
    "- Compared **four classifiers** fairly via **5‑fold CV**\n",
    "- Picked the best and validated on a **hold‑out test set**\n",
    "- Added **explainability** (SHAP for linear model)\n",
    "- Saved a **single deployment artifact** (vectorizer + classifier)\n",
    "\n",
    "**Next ideas**\n",
    "- Hyperparameter search (e.g., `RandomizedSearchCV`) to squeeze more performance\n",
    "- Add **n‑gram/feature selection** sweeps, or character n‑grams for robustness\n",
    "- Explore class imbalance handling beyond `class_weight` (e.g., threshold tuning)\n",
    "- Temporal analysis if `Date` exists (trend lines, drift detection)\n",
    "- Add a tiny **Streamlit** app using the saved pipeline for demo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
